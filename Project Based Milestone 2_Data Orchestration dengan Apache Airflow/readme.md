# ğŸ“Š Data Orchestration dengan Apache Airflow

## ğŸ¯ Tujuan
Mengubah implementasi Data Warehouse DuckDB dan pipeline ETL dari tugas sebelumnya menjadi alur kerja yang siap produksi dan terorkestrasi menggunakan **Apache Airflow**. Tugas ini mempersiapkan pipeline data Anda untuk keperluan **visualisasi** dan **penggunaan operasional** lainnya.

---

## ğŸ§± Bagian 1: Menyiapkan Lingkungan Airflow

- Siapkan lingkungan Airflow lokal (menggunakan **Docker** atau instalasi langsung)
- Konfigurasikan koneksi yang diperlukan untuk **sumber data** dan **DuckDB**

---

## ğŸ› ï¸ Bagian 2: Desain dan Implementasi DAG

### 2.1 DAG Pipeline ETL

Konversi pipeline ETL sebelumnya menjadi DAG di Airflow:

- Implementasikan dependensi tugas menggunakan:
  - Operator tradisional (`>>` dan `<<`), atau
  - TaskFlow API dengan `@task` decorators (opsional)
- Komponen DAG:
  - ğŸ”„ **Ekstraksi** dari setiap sumber data
  - ğŸ§ª **Transformasi** sesuai definisi ETL
  - ğŸ“¥ **Loading** ke dalam data warehouse DuckDB
  - ğŸ‘€ Setidaknya satu **Sensor** untuk cek ketersediaan data

### 2.2 Fitur Airflow Lanjutan

Implementasikan setidaknya **dua** fitur lanjutan berikut:

- ğŸ”€ Branching logic berdasarkan kondisi
- â™»ï¸ Dynamic task generation
- ğŸ§© Custom operators untuk logika bisnis tertentu
- âŒ Error handling dan mekanisme retry
- ğŸ“£ Notifikasi (Email/Slack) untuk status berhasil/gagal
- â± SLA dan pemantauan
- âª Backfilling capabilities

### 2.3 Strategi Penjadwalan dan Partisi

- Rancang strategi **penjadwalan** DAG yang sesuai
- Dokumentasikan keputusan penjadwalan dan **alasannya**

---

## ğŸ§ª Bagian 3: Pengujian dan Dokumentasi

### 3.1 Pengujian DAG

- Uji DAG menggunakan fitur testing Airflow
- Sertakan log eksekusi yang berhasil

### 3.2 Dokumentasi

Dokumentasi wajib meliputi:

- ğŸ—º Diagram arsitektur DAG & dependensi
- ğŸ“Œ Deskripsi tujuan setiap task
- ğŸ•’ Informasi penjadwalan & dependensi
- ğŸ‘ï¸â€ğŸ—¨ï¸ Pengaturan pemantauan & peringatan
- ğŸ” Prosedur pemulihan kegagalan

---

## ğŸ“ Bagian 4: Kualitas Data

- Implementasikan minimal **dua pemeriksaan kualitas data** menggunakan:
  - Airflow
  - Great Expectations
  - Python Native
- Buat DAG **terpisah** untuk memantau kualitas data
- Dokumentasikan:
  - Metrik kualitas data
  - Ambang batas (threshold)

---

## ğŸ“¦ Deliverable

### ğŸ—‚ Repositori Kode
- File definisi DAG Airflow
- Script Python untuk ETL, sensor, dan validasi

### ğŸ“š Dokumentasi
- Diagram arsitektur DAG
- Dokumen strategi penjadwalan
- Dokumen metrik kualitas data

### ğŸ§¾ Bukti Eksekusi
- ğŸ“¸ Tangkapan layar DAG yang sukses dijalankan
- ğŸ“„ Log eksekusi yang valid
- ğŸ“¸ Tampilan UI Airflow yang menampilkan DAG

---

## ğŸ“Š Rencana Persiapan Visualisasi

- Ringkasan kesiapan data untuk divisualisasikan
- Rekomendasi alat visualisasi:
  - Contoh: **Metabase**, **Apache Superset**, atau **Power BI**
- Contoh kueri yang berguna untuk visualisasi:
  ```sql
  SELECT customer_id, SUM(order_total) as total_spent
  FROM orders
  GROUP BY customer_id
  ORDER BY total_spent DESC
  LIMIT 10;

